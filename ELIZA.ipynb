{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import datetime as dt\n",
    "from dateutil import parser \n",
    "import re\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from utilities.loaders import read_files, read_rtf_files, read_json_files, read_csv_files, read_doc_files, read_xlsx_files, read_txt_files\n",
    "\n",
    "# for preproessing json data\n",
    "from utilities.preprocessors import normalize_and_clean, extract_keys_values, clean_and_split_data, extract_keys_values, cohere_csvs, cohere\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dataset#5938README.md']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dir = './ELIZA input'\n",
    "files = os.listdir(input_dir)\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dataset#5938README': ['---\\n',\n",
       "  'language:\\n',\n",
       "  '- en\\n',\n",
       "  'license: mit\\n',\n",
       "  'size_categories:\\n',\n",
       "  '- 100K<n<1M\\n',\n",
       "  'task_categories:\\n',\n",
       "  '- text-generation\\n',\n",
       "  'pretty_name: UltraChat 200k\\n',\n",
       "  'configs:\\n',\n",
       "  '- config_name: default\\n',\n",
       "  '  data_files:\\n',\n",
       "  '  - split: train_sft\\n',\n",
       "  '    path: data/train_sft-*\\n',\n",
       "  '  - split: test_sft\\n',\n",
       "  '    path: data/test_sft-*\\n',\n",
       "  '  - split: train_gen\\n',\n",
       "  '    path: data/train_gen-*\\n',\n",
       "  '  - split: test_gen\\n',\n",
       "  '    path: data/test_gen-*\\n',\n",
       "  'dataset_info:\\n',\n",
       "  '  features:\\n',\n",
       "  '  - name: prompt\\n',\n",
       "  '    dtype: string\\n',\n",
       "  '  - name: prompt_id\\n',\n",
       "  '    dtype: string\\n',\n",
       "  '  - name: messages\\n',\n",
       "  '    list:\\n',\n",
       "  '    - name: content\\n',\n",
       "  '      dtype: string\\n',\n",
       "  '    - name: role\\n',\n",
       "  '      dtype: string\\n',\n",
       "  '  splits:\\n',\n",
       "  '  - name: train_sft\\n',\n",
       "  '    num_bytes: 1397058554\\n',\n",
       "  '    num_examples: 207865\\n',\n",
       "  '  - name: test_sft\\n',\n",
       "  '    num_bytes: 154695659\\n',\n",
       "  '    num_examples: 23110\\n',\n",
       "  '  - name: train_gen\\n',\n",
       "  '    num_bytes: 1347396812\\n',\n",
       "  '    num_examples: 256032\\n',\n",
       "  '  - name: test_gen\\n',\n",
       "  '    num_bytes: 148276089\\n',\n",
       "  '    num_examples: 28304\\n',\n",
       "  '  download_size: 1624049723\\n',\n",
       "  '  dataset_size: 3047427114\\n',\n",
       "  '---\\n',\n",
       "  '\\n',\n",
       "  '# Dataset Card for UltraChat 200k\\n',\n",
       "  '\\n',\n",
       "  '## Dataset Description\\n',\n",
       "  '\\n',\n",
       "  'This is a heavily filtered version of the [UltraChat](https://github.com/thunlp/UltraChat) dataset and was used to train [Zephyr-7B-Î²](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta), a state of the art 7b chat model.\\n',\n",
       "  '\\n',\n",
       "  'The original datasets consists of 1.4M dialogues generated by ChatGPT and spanning a wide range of topics. To create `UltraChat 200k`, we applied the following logic:\\n',\n",
       "  '\\n',\n",
       "  '- Selection of a subset of data for faster supervised fine tuning.\\n',\n",
       "  '- Truecasing of the dataset, as we observed around 5% of the data contained grammatical errors like \"Hello. how are you?\" instead of \"Hello. How are you?\"\\n',\n",
       "  '- Removal of dialogues where the assistant replies with phrases like \"I do not have emotions\" or \"I don\\'t have opinions\", even for fact-based prompts that don\\'t involve either.\\n',\n",
       "  '\\n',\n",
       "  '## Dataset Structure\\n',\n",
       "  '\\n',\n",
       "  'The dataset has four splits, suitable for:\\n',\n",
       "  '\\n',\n",
       "  '* Supervised fine-tuning (`sft`).\\n',\n",
       "  '* Generation ranking (`gen`) via techniques like rejection sampling or PPO.\\n',\n",
       "  '\\n',\n",
       "  'The number of examples per split is shown as follows:\\n',\n",
       "  '\\n',\n",
       "  '\\n',\n",
       "  '|  train_sft | test_sft  | train_gen | test_gen |\\n',\n",
       "  '|:-------:|:-----------:|:-----:| :-----:|\\n',\n",
       "  '|  207865 |       23110 | 256032 | 28304 |\\n',\n",
       "  '\\n',\n",
       "  'The dataset is stored in parquet format with each entry using the following schema:\\n',\n",
       "  '```\\n',\n",
       "  '\\n',\n",
       "  '{\\n',\n",
       "  '    \"prompt\": \"Create a fully-developed protagonist who is challenged to survive within a dystopian society under the rule of a tyrant. ...\",\\n',\n",
       "  '    \"messages\":[\\n',\n",
       "  '        {\\n',\n",
       "  '            \"content\": \"Create a fully-developed protagonist who is challenged to survive within a dystopian society under the rule of a tyrant. ...\",\\n',\n",
       "  '            \"role\": \"user\"\\n',\n",
       "  '        },\\n',\n",
       "  '        {\\n',\n",
       "  '            \"content\": \"Name: Ava\\\\n\\\\n Ava was just 16 years old when the world as she knew it came crashing down. The government had collapsed, leaving behind a chaotic and lawless society. ...\",\\n',\n",
       "  '            \"role\": \"assistant\"\\n',\n",
       "  '        },\\n',\n",
       "  '        {\\n',\n",
       "  '            \"content\": \"Wow, Ava\\'s story is so intense and inspiring! Can you provide me with more details.  ...\",\\n',\n",
       "  '            \"role\": \"user\"\\n',\n",
       "  '        },\\n',\n",
       "  '        {\\n',\n",
       "  '            \"content\": \"Certainly! ....\",\\n',\n",
       "  '            \"role\": \"assistant\"\\n',\n",
       "  '        },\\n',\n",
       "  '        {\\n',\n",
       "  '            \"content\": \"That\\'s really interesting! I would love to hear more...\",\\n',\n",
       "  '            \"role\": \"user\"\\n',\n",
       "  '        }\\n',\n",
       "  '        {\\n',\n",
       "  '            \"content\": \"Certainly! ....\",\\n',\n",
       "  '            \"role\": \"assistant\"\\n',\n",
       "  '        },\\n',\n",
       "  '    ],\\n',\n",
       "  '    \"prompt_id\": \"d938b65dfe31f05f80eb8572964c6673eddbd68eff3db6bd234d7f1e3b86c2af\"\\n',\n",
       "  '}\\n',\n",
       "  '```\\n',\n",
       "  '\\n',\n",
       "  '## Citation\\n',\n",
       "  '\\n',\n",
       "  'If you find this dataset is useful in your work, please cite the original UltraChat dataset:\\n',\n",
       "  '\\n',\n",
       "  '```\\n',\n",
       "  '@misc{ding2023enhancing,\\n',\n",
       "  '      title={Enhancing Chat Language Models by Scaling High-quality Instructional Conversations}, \\n',\n",
       "  '      author={Ning Ding and Yulin Chen and Bokai Xu and Yujia Qin and Zhi Zheng and Shengding Hu and Zhiyuan Liu and Maosong Sun and Bowen Zhou},\\n',\n",
       "  '      year={2023},\\n',\n",
       "  '      eprint={2305.14233},\\n',\n",
       "  '      archivePrefix={arXiv},\\n',\n",
       "  '      primaryClass={cs.CL}\\n',\n",
       "  '}\\n',\n",
       "  '```']}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "md_files = [file for file in files if file.endswith('.md')]\n",
    "mds = read_txt_files(input_dir, md_files)\n",
    "mds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'en', 'license mit', 'size categories', 'n m', 'task categories', 'text generation', 'pretty name ultrachat', 'configs', 'config name default', 'data files', 'split train sft', 'path data train sft', 'split test sft', 'path data test sft', 'split train gen', 'path data train gen', 'split test gen', 'path data test gen', 'dataset info', 'features', 'name prompt', 'dtype string', 'name prompt id', 'dtype string', 'name messages', 'list', 'name content', 'dtype string', 'name role', 'dtype string', 'splits', 'name train sft', 'num bytes', 'num examples', 'name test sft', 'num bytes', 'num examples', 'name train gen', 'num bytes', 'num examples', 'name test gen', 'num bytes', 'num examples', 'download size', 'dataset size', 'dataset card for ultrachat', 'dataset description', 'this is a heavily filtered version of the ultrachat thunlp ultrachat dataset and was used to train zephyr b huggingfaceh zephyr b beta a state of the art b chat model', 'the original datasets consists of m dialogues generated by chatgpt and spanning a wide range of topics to create ultrachat we applied the following logic', 'selection of a subset of data for faster supervised fine tuning', 'truecasing of the dataset as we observed around percent of the data contained grammatical errors like hello how are you instead of hello how are you', 'removal of dialogues where the assistant replies with phrases like i do not have emotions or i do not have opinions even for fact based prompts that do not involve either', 'dataset structure', 'the dataset has four splits suitable for', 'supervised fine tuning sft', 'generation ranking gen via techniques like rejection sampling or ppo', 'the number of examples per split is shown as follows', 'train sft test sft train gen test gen', 'the dataset is stored in parquet format with each entry using the following schema', 'prompt create a fully developed protagonist who is challenged to survive within a dystopian society under the rule of a tyrant', 'messages', 'content create a fully developed protagonist who is challenged to survive within a dystopian society under the rule of a tyrant', 'role user', 'content name ava n n ava was just years old when the world as she knew it came crashing down the government had collapsed leaving behind a chaotic and lawless society', 'role assistant', 'content wow ava story is so intense and inspiring can you provide me with more details', 'role user', 'content certainly', 'role assistant', 'content that is really interesting i would love to hear more', 'role user', 'content certainly', 'role assistant', 'prompt id d b dfe f f eb c eddbd eff3db6bd d7f1e3b c2af', 'citation', 'if you find this dataset is useful in your work please cite the original ultrachat dataset', 'misc ding enhancing', 'title enhancing chat language models by scaling high quality instructional conversations', 'author ning ding and yulin chen and bokai xu and yujia qin and zhi zheng and shengding hu and zhiyuan liu and maosong sun and bowen zhou', 'year', 'eprint', 'archiveprefix arxiv', 'primaryclass']\n",
      "output file: ./ELIZA output\\Dataset#5938README_processed_part1.txt\n",
      "output file: ./ELIZA output\\Dataset#5938README_processed_part2.txt\n",
      "output file: ./ELIZA output\\Dataset#5938README_processed_part3.txt\n",
      "File ./ELIZA output\\Dataset#5938README_processed_part3.txt has been created with 4 lines.\n"
     ]
    }
   ],
   "source": [
    "output_dir = './ELIZA output'\n",
    "for name, lists in mds.items():\n",
    "    clean_and_split_data(name, lists, output_dir=output_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtuals-internship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
